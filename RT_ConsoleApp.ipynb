{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyCH2tFBSVeB"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dseZBwpuST5T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIGeWIFbSf1B"
      },
      "source": [
        "# Google Drive and Device Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84nVof-VSnNW",
        "outputId": "204a63df-398b-47b8-e94a-26ff9c402fba"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_sO2UynSp_U"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T9_SMDG7SsiC"
      },
      "outputs": [],
      "source": [
        "input_dim = 120  # 40 MFCC + 40 delta + 40 delta-delta\n",
        "hidden_dim = 256\n",
        "output_dim = 40  # Original MFCC dimension\n",
        "num_layers = 2\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhCEB9rwTGcz"
      },
      "source": [
        "# RVC Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NcEAJiZdTGEZ"
      },
      "outputs": [],
      "source": [
        "class RVCModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(RVCModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, _ = self.lstm(packed)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        out = self.linear(output)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wce7Trb2TY0w"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iR-77PQTTZhR"
      },
      "outputs": [],
      "source": [
        "def extract_features(wav_file):\n",
        "    waveform, sample_rate = torchaudio.load(wav_file)\n",
        "    mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=40)(waveform)\n",
        "    delta = torchaudio.functional.compute_deltas(mfcc)\n",
        "    delta2 = torchaudio.functional.compute_deltas(delta)\n",
        "    features = torch.cat([mfcc, delta, delta2], dim=1)\n",
        "    return features.squeeze(0).T.numpy()  # (time, features)\n",
        "\n",
        "def prepare_data(audio_folder):\n",
        "    features_list = []\n",
        "    lengths = []\n",
        "    for file in os.listdir(audio_folder):\n",
        "        if file.endswith('.wav'):\n",
        "            wav_path = os.path.join(audio_folder, file)\n",
        "            features = extract_features(wav_path)\n",
        "            features_list.append(features)\n",
        "            lengths.append(features.shape[0])\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_len = max(lengths)\n",
        "    padded_features = [np.pad(f, ((0, max_len - f.shape[0]), (0, 0)), mode='constant') for f in features_list]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    features_tensor = torch.tensor(np.array(padded_features), dtype=torch.float32)\n",
        "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
        "    return features_tensor, lengths_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaxIlHX7UL9h"
      },
      "source": [
        "# Training the RVC Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4xyx2oc5ULx-"
      },
      "outputs": [],
      "source": [
        "def train_model(source_features, source_lengths, target_features, target_lengths):\n",
        "    # Split data into train and test sets\n",
        "    train_source, test_source, train_target, test_target, train_source_lengths, test_source_lengths = train_test_split(\n",
        "        source_features, target_features, source_lengths, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = RVCModel(input_dim, hidden_dim, output_dim, num_layers).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    n_batches = len(train_source) // batch_size\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "\n",
        "            batch_source = train_source[start_idx:end_idx].to(device)\n",
        "            batch_target = train_target[start_idx:end_idx, :, :output_dim].to(device)  # Only use the first 40 features\n",
        "            batch_lengths = train_source_lengths[start_idx:end_idx]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_source, batch_lengths)\n",
        "\n",
        "            # Ensure output and target have the same time steps\n",
        "            min_time_steps = min(output.size(1), batch_target.size(1))\n",
        "            output = output[:, :min_time_steps, :]\n",
        "            batch_target = batch_target[:, :min_time_steps, :]\n",
        "\n",
        "            loss = loss_fn(output, batch_target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / n_batches\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch: {epoch}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_output = model(test_source.to(device), test_source_lengths)\n",
        "            test_target = test_target[:, :, :output_dim].to(device)  # Only use the first 40 features\n",
        "            min_time_steps = min(test_output.size(1), test_target.size(1))\n",
        "            test_output = test_output[:, :min_time_steps, :]\n",
        "            test_target = test_target[:, :min_time_steps, :]\n",
        "            test_loss = loss_fn(test_output, test_target)\n",
        "            print(f'Test Loss: {test_loss.item():.4f}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PD-dT4rUUBu"
      },
      "source": [
        "# Voice conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lIlBNEjcUUXs"
      },
      "outputs": [],
      "source": [
        "def convert_voice(model, source_features, source_lengths):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        converted_features = model(source_features.to(device), source_lengths)\n",
        "    return converted_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RegDZW9GdOAF"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIOBT9pldSLX",
        "outputId": "afbfc7d7-3313-4ad6-8e7d-ec52678a8b76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source features shape: torch.Size([137, 5289, 120])\n",
            "Target features shape: torch.Size([137, 4734, 120])\n",
            "Source lengths shape: torch.Size([137])\n",
            "Target lengths shape: torch.Size([137])\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "source_folder = 'C:\\\\Users\\\\User\\\\Downloads\\\\obama_dataset\\\\source_data'\n",
        "target_folder = 'C:\\\\Users\\\\User\\\\Downloads\\\\obama_dataset\\\\target_data'\n",
        "# output_folder = 'path/to/output_folder'\n",
        "\n",
        "# Prepare data\n",
        "source_features, source_lengths = prepare_data(source_folder)\n",
        "target_features, target_lengths = prepare_data(target_folder)\n",
        "\n",
        "print(\"Source features shape:\", source_features.shape)\n",
        "print(\"Target features shape:\", target_features.shape)\n",
        "print(\"Source lengths shape:\", source_lengths.shape)\n",
        "print(\"Target lengths shape:\", target_lengths.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdhTmerTBsnZ"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NlcpiLuEB2ln"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Average Loss: 344.5980\n",
            "Test Loss: 374.4282\n",
            "Test Loss: 368.6375\n",
            "Test Loss: 363.9091\n",
            "Test Loss: 360.2078\n",
            "Test Loss: 357.2200\n",
            "Test Loss: 354.7082\n",
            "Test Loss: 352.4995\n",
            "Test Loss: 350.4926\n",
            "Test Loss: 348.6634\n",
            "Test Loss: 346.9780\n",
            "Epoch: 10, Average Loss: 315.9451\n",
            "Test Loss: 345.4282\n",
            "Test Loss: 343.9987\n",
            "Test Loss: 342.6696\n",
            "Test Loss: 341.4384\n",
            "Test Loss: 340.2976\n",
            "Test Loss: 339.2351\n",
            "Test Loss: 338.2326\n",
            "Test Loss: 337.2787\n",
            "Test Loss: 336.3721\n",
            "Test Loss: 335.5155\n",
            "Epoch: 20, Average Loss: 305.6469\n",
            "Test Loss: 334.7023\n",
            "Test Loss: 333.9244\n",
            "Test Loss: 333.1805\n",
            "Test Loss: 332.4727\n",
            "Test Loss: 331.7946\n",
            "Test Loss: 331.1429\n",
            "Test Loss: 330.5148\n",
            "Test Loss: 329.9167\n",
            "Test Loss: 329.3419\n",
            "Test Loss: 328.7873\n",
            "Epoch: 30, Average Loss: 299.4725\n",
            "Test Loss: 328.2542\n",
            "Test Loss: 327.7427\n",
            "Test Loss: 327.2535\n",
            "Test Loss: 326.7849\n",
            "Test Loss: 326.3374\n",
            "Test Loss: 325.9100\n",
            "Test Loss: 325.5022\n",
            "Test Loss: 325.1134\n",
            "Test Loss: 324.7434\n",
            "Test Loss: 324.3911\n",
            "Epoch: 40, Average Loss: 295.6133\n",
            "Test Loss: 324.0556\n",
            "Test Loss: 323.7364\n",
            "Test Loss: 323.4324\n",
            "Test Loss: 323.1434\n",
            "Test Loss: 322.8685\n",
            "Test Loss: 322.6072\n",
            "Test Loss: 322.3588\n",
            "Test Loss: 322.1227\n",
            "Test Loss: 321.8984\n",
            "Test Loss: 321.6855\n",
            "Epoch: 50, Average Loss: 293.3215\n",
            "Test Loss: 321.4834\n",
            "Test Loss: 321.2917\n",
            "Test Loss: 321.1099\n",
            "Test Loss: 320.9374\n",
            "Test Loss: 320.7740\n",
            "Test Loss: 320.6190\n",
            "Test Loss: 320.4723\n",
            "Test Loss: 320.3333\n",
            "Test Loss: 320.2019\n",
            "Test Loss: 320.0774\n",
            "Epoch: 60, Average Loss: 292.0325\n",
            "Test Loss: 319.9597\n",
            "Test Loss: 319.8483\n",
            "Test Loss: 319.7429\n",
            "Test Loss: 319.6433\n",
            "Test Loss: 319.5492\n",
            "Test Loss: 319.4601\n",
            "Test Loss: 319.3760\n",
            "Test Loss: 319.2964\n",
            "Test Loss: 319.2212\n",
            "Test Loss: 319.1501\n",
            "Epoch: 70, Average Loss: 291.3424\n",
            "Test Loss: 319.0829\n",
            "Test Loss: 319.0194\n",
            "Test Loss: 318.9593\n",
            "Test Loss: 318.9025\n",
            "Test Loss: 318.8488\n",
            "Test Loss: 318.7980\n",
            "Test Loss: 318.7499\n",
            "Test Loss: 318.7046\n",
            "Test Loss: 318.6616\n",
            "Test Loss: 318.6208\n",
            "Epoch: 80, Average Loss: 290.9829\n",
            "Test Loss: 318.5822\n",
            "Test Loss: 318.5457\n",
            "Test Loss: 318.5124\n",
            "Test Loss: 318.5128\n",
            "Test Loss: 318.4781\n",
            "Test Loss: 318.4449\n",
            "Test Loss: 318.4146\n",
            "Test Loss: 318.3884\n",
            "Test Loss: 318.3619\n",
            "Test Loss: 318.4039\n",
            "Epoch: 90, Average Loss: 290.8308\n",
            "Test Loss: 318.3756\n",
            "Test Loss: 318.3601\n",
            "Test Loss: 318.3292\n",
            "Test Loss: 318.3150\n",
            "Test Loss: 318.3521\n",
            "Test Loss: 318.3322\n",
            "Test Loss: 318.2838\n",
            "Test Loss: 318.2940\n",
            "Test Loss: 318.2837\n",
            "Test Loss: 318.3358\n",
            "Voice conversion completed.\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "rvc_model = train_model(source_features, source_lengths, target_features, target_lengths)\n",
        "\n",
        "drive_save_path = 'C:\\\\Users\\\\User\\\\Downloads\\\\obama_dataset\\\\rvc_model.pth'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(drive_save_path), exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "torch.save(rvc_model.state_dict(), drive_save_path)\n",
        "\n",
        "# Convert voice (example)\n",
        "sample_source = source_features[0].unsqueeze(0)  # Add batch dimension\n",
        "sample_length = source_lengths[0].unsqueeze(0)\n",
        "converted_features = convert_voice(rvc_model, sample_source, sample_length)\n",
        "\n",
        "print(\"Voice conversion completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert Voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_file = \"C:\\\\Users\\\\User\\\\Downloads\\\\audio1.wav\\\\\"\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
